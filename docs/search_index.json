[["index.html", "Interprete o seu modelo caixa-preta! Capítulo 1 Sobre esse material 1.1 Autores 1.2 Alinhando Expectativas", " Interprete o seu modelo caixa-preta! Última atualização: 08/10/2021 Capítulo 1 Sobre esse material Esse é um conteúdo sobre técnicas de interpretabilidade, iniciado durante o desenvolvimento da minha dissertação de mestrado, sob orientação do Professor Ronaldo Cristiano Prati, realizado na Universidade Federal do ABC. 1.1 Autores Angélica C. Cruz Custódio Ronaldo Cristiano Prati E quem mais quiser contribuir - o conteúdo está aberto no github e contribuições são bem-vindas :) 1.2 Alinhando Expectativas O objetivo desse material é apresentar algumas ferramentas de interpretabilidade utilizadas na interpretação de predições. Acredito que essas ferramentas são aliadas importantes quando utilizamos modelos caixa-preta, nos possibilitando sair do lugar no qual não enxergamos a relação das variáveis explicativas com o predito e nos leva para um lugar de questionamentos, análises e possíveis compreensões. Esse conteúdo é um guia para quem tem interesse nesse tópico - importante e por vezes pouco explorado - dentro do universo de Ciência de Dados. “Nada na vida deve ser temido, somente compreendido. Agora é hora de compreender mais para temer menos.” - Marie Curie "],["intro.html", "Capítulo 2 Introdução 2.1 Algoritmo de Contratação sexista 2.2 Algoritmo de classificação de imagens", " Capítulo 2 Introdução Considerar a interpretabilidade é muito importante quando falamos de predições e classificações resultantes de um modelo. Buscar a interpretação de uma predição nos ajuda a entender como chegamos naquele resultado, quais características impactaram nele, além de nos ajudar a tomar decisões com mais responsabilidade e segurança. Ao falarmos de interpretabilidade, uma possível abordagem é utilizar técnicas de modelagem que permitem entender diretamente como as variáveis explicativas se relacionam com a variável resposta: os Modelos Interpretáveis. Regressão Linear e Regressão Logística (Molnar 2019) são algumas dessas técnicas que podem ser consideradas interpretáveis, dependendo do nível de complexidade na combinação das suas variáveis, das regras e também do nível de domínio do assunto de quem está analisando-as (Doshi-Velez 2017). Figura 2.1: Fonte: Capgemini - Explainable AI- why is there a need to explain? Entretanto, quando utilizamos modelos não interpretáveis, também conhecidos como Modelos Opacos ou “Caixa Preta”, não é possível entender diretamente a relação entre as variáveis explicativas e a variável resposta. Nesses modelos a predição pode ser resultado de um conjunto complexo de regras ou as relações não se apresentam de forma direta e linear, como em Florestas Aleatórias (Breiman 2001), as Máquinas de Vetores de Suporte (Shawe-Taylor and Cristianini 2000) (SVM, do termo em inglês Support Vector Machine), e as Redes Neurais Profundas (Liu et al. 2017). Nesses casos não fica claro o impacto de cada variável e, como consequência, podemos ter uma série de problemas com variáveis que não deveriam ser relevantes e erroneamente são consideradas pelo modelo, seja por um viés de seleção das amostras ou por um viés histórico sócio-cultural que acaba refletindo na base de dados utilizada no treinamento do modelo. Esse cenário nos possibilita tomar decisões com base em predições enviesadas sem sequer saber disso, o que pode levar a repetição de padrões e processos discriminatórios que impactam diretamente na desigualdade entre indivíduos, como foi abordado por Cathy O’Neil em (O’Neil 2016). A discussão sobre viés em algoritmos está permeada por muitas questões éticas e morais. Como apontado em (“Algoritmos de Inteligência Artificial E Vieses: Uma Reflexão Sobre ética E Justiça” 2020), essa discussão implica falar sobre questões como gênero, raça, idade, orientação afetiva-sexual, linguagem, cultura, deficiência, condição econômica, entre outros fatores e suas interseccionalidades. Para ilustrar algumas dessas questões, trouxemos exemplos reais em que, por não terem considerado a etapa de interpretabilidade no processo de desenvolvimento do modelo, implica como resultado classificações incorretas e com impactos negativos, devido aos vieses na informações do treinamento do algoritmo. 2.1 Algoritmo de Contratação sexista Segundo a agência Reuters (Reuters 2018), com a proposta de elevar o nível de tomada de decisão automática, a empresa Amazon desenvolveu internamente uma ferramenta experimental de contratação utilizando inteligência artificial para fornecer aos candidatos pontuações que variam de uma a cinco estrelas. Entretanto, a empresa percebeu que seu novo método não classificava os candidatos de forma neutra em termos de gênero. Isso ocorreu porque, para classificar os candidatos, o modelo foi treinado utilizando a base histórica dos currículos enviados à empresa nos últimos 10 anos. Nessa base era refletida a dominância masculina que existe em toda a indústria de tecnologia. Figura 2.2: Fonte: MIT Sloan - Could AI Be the Cure for Workplace Gender Inequality? De acordo com a reportagem da The Guardian (Guardian 2018), cerca de 55% dos gerentes de recursos humanos dos EUA disseram que a inteligência artificial seria uma parte regular de seu trabalho nos próximos cinco anos, segundo uma pesquisa de 2017 realizada pela empresa de software de talentos CareerBuilder. Esse tipo de experimento que a (Reuters 2018) relatou, é um exemplo sobre as implicações éticas que um modelo pode ter e o quão importante e urgente é a preocupação de interpretar o modelo. 2.2 Algoritmo de classificação de imagens Em (Lapuschkin et al. 2019) foi estudado um exemplo em que o algoritmo de aprendizado de máquina encontrou uma característica curiosa nas imagens para classificá-las. O objetivo era classificação entre cavalo, pessoa, trem ou carro. Entretanto, para classificar como cavalo o algoritmo considera como área relevante a marca d’água do fotógrafo que estava na base de treinamento do modelo. Isso justifica porque, na base de teste, uma imagem de carro que contém a marca d’água é classificada como cavalo, como visto na figura abaixo. Figura 2.3: Fonte: Nesse caso, a marca d’água do fotógrafo na base de treinamento atrapalhou a classificação, e quando observamos na imagem quais áreas foram mais relevantes para aquela rotulação como cavalo encontramos a resposta do viés do algoritmo. References "],["caracteristicas.html", "Capítulo 3 Características dos métodos de interpretabilidade", " Capítulo 3 Características dos métodos de interpretabilidade Uma possível abordagem para interpretabilidade de modelos de aprendizado de máquina que não são entendidos diretamente por suas fórmulas, os chamados modelos caixa preta, é usar métodos modelo-agnósticos (Ribeiro, Singh, and Guestrin 2016) para interpretabilidade. Essa abordagem consiste em extrair explicações do modelo ajustado, considerando os resultados da predição/classificação para fazer análises. As análises se dão pela modificação de variáveis de entradas e observação de como o modelo se altera em cada uma das modificações realizadas. Antes de abordar os métodos estudados nesse trabalho, traremos a seguir três atributos que caracterizam esses métodos: Flexibilidade do Modelo, Flexibilidade de Explicação e Flexibilidade de Representação. Flexibilidade do Modelo Separar a etapa de interpretabilidade da etapa de ajuste do modelo permite que o modelo seja o mais flexível possível, pois possibilita o uso de qualquer abordagem de modelagem. Se a abordagem do problema for Florestas Aleatórias ou se for SVMs, por exemplo, o método de interpretação utilizado será o mesmo. Esse ponto leva à discussão o interpretabilidade complexidade do modelo, uma vez que abordamos modelos mais ou menos complexos com os mesmo métodos de interpretabilidade. Flexibilidade de Explicação Diferentes tipos de problemas e necessidades levam à diferentes tipos de explicações. Em alguns casos, os usuários podem se preocupar em entender como os resultados positivos impactam em uma determinada previsão. Por exemplo, qual parte de um imagem é mais responsável pela previsão (exemplo do cavalo e do automóvel), enquanto em outros casos em que o impacto negativo pode ser o interesse, por exemplo, na depuração de um classificador de contratação (exemplo do Classificador Sexista de contratação). Temos também os casos em que a necessidade de informação pode ser também contrafactual, por exemplo, como o modelo se comportaria se certas características tivessem valores diferentes. Usuários diferentes também podem lidar com diferentes tipos de explicações. Um leigo pode ser capaz de entender uma Árvore de Decisão com um número um pouco maior de regras, enquanto um modelo linear é mais intuitivo para um usuário acostumado com modelos estatísticos. No entanto, a maioria dos modelos interpretáveis são restritos nas possíveis explicações, seja por um conjunto de regras ou por gráficos. Assim, mantendo o modelo separado das explicações, é possível adaptar a explicação às necessidades de informações, mantendo o modelo fixo, ou seja, independente do modelo abordado, ele pode ser explicado de diferentes maneira e diferentes graus de interpretabilidade. Flexibilidade de Representação A característica de Flexibilidade na Representação está relacionada a gerar explicações usando recursos diferentes, de acordo com o modelo que está sendo explicado. Por exemplo, para um classificador de texto que usa vetores de incorporação de palavras abstratas, pode ser preferível usar a presença de palavras individuais para a explicação (Ribeiro, Singh, and Guestrin 2016). Com as características essenciais aos métodos de interpretabilidade abordadas, estudamos alguns métodos utilizados, suas definições e como eles ajudam a entender a relação do predito com as variáveis explicativas. Nas próximas seções descrevemos alguns métodos de interpretabilidade que podem ser aplicados a qualquer modelo de predição, abordando uma perspectiva de interpretabilidade Global e, posteriormente, Local. References "],["interpglobal.html", "Capítulo 4 Interpretabilidade Global 4.1 Gráfico de Dependência Parcial (Partial Dependence Plot) - PDP 4.2 Esperança Condicional Individual (Individual Conditional Expectation) - ICE 4.3 Efeitos Acumulados Locais (Accumulated Local Effects) - ALE", " Capítulo 4 Interpretabilidade Global Algumas técnicas de interpretabilidade de modelos opacos facilitam o entendimento dos seus resultados de uma perspectiva global, ou seja, as relações aprendidas entre a variável resposta e as variáveis explicativas do ponto de vista de um conjunto de dados e o modelo previamente ajustado. 4.1 Gráfico de Dependência Parcial (Partial Dependence Plot) - PDP O gráfico de dependência parcial mostra um resumo da relação de uma ou duas variáveis explicativas do modelo e sua dependência com o predito, por meio do efeito marginal. Com ele podemos observar, por exemplo, se existe uma relação entre as variáveis e se ela é linear, quadrática ou mais complexa (Friedman 2001). No caso de construir o gráfico com uma variável explicativa temos os valores preditos pelo modelo plotados para cada valor correspondente da variável explicativa. Se a variável for categórica, temos um gráfico de barras em que cada barra representa uma categoria diferente. Quando construímos com duas variáveis explicativas temos um gráfico tri-dimensional, e podemos analisar pelas diferentes perspectivas dele para entender a relação com os valores preditos. Nessa análise não é comum considerarmos mais que duas variáveis, pois precisaríamos avaliar mais que três dimensões. Para definirmos o PDP temos \\(x_S\\), um subconjunto das variáveis explicativas \\(X\\) que utilizaremos no gráfico. Como já citamos anteriormente, geralmente esse subconjunto é de uma ou duas variáveis. \\[x_S \\in X\\] e \\(x_C\\) o subconjunto complementar das variáveis utilizadas no ajuste do modelo \\[x_S \\cup x_C= X.\\] Como esperado, o predito do modelo \\(\\hat{f}\\) depende dos dois subconjuntos de variáveis \\[\\hat{f}(X) = \\hat{f}(x_S, x_C).\\] O conceito da dependência parcial baseia-se em marginalizar o predito pelo modelo sobre a distribuição das variáveis no subconjunto \\(C\\), de modo que a função mostre a relação entre o predito e as variáveis no conjunto \\(S\\) nas quais estamos interessados. A função de dependência parcial para regressão é definida como: \\[\\hat{f}_{x_S} (x_S) = E_{x_C }[\\hat{f}(X)] = \\int \\hat{f}(x_S,x_C) \\delta P(x_C) .\\] Ao marginalizar sobre as variáveis do conjunto \\(C\\), temos uma função que depende apenas das variáveis em \\(S\\). Assim, observamos que a função parcial nos diz: para determinado valor das características \\(S\\) qual é o efeito marginal médio na previsão do modelo. 4.1.1 Vantagens e Desvantagens A fácil implementação do PDP é uma de suas vantagens. Matematicamente ele não é uma método complexo. Além disso o gráfico é intuitivo e simples de ser interpretado. Por outro lado, com o PDP, não temos a noção da distribuição da variável, o que pode levar a tirar conclusões sobre um intervalo que tem um baixo volume de dados na base originalmente considerada no desenvolvimento do modelo. Outro ponto importante é a suposição de independência: supõe-se que as variáveis para as quais a dependência parcial é calculada não estão correlacionados com outras variáveis. Por exemplo, temos que prever o número de sapatos vendidos por um período, considerando idade, altura e tamanho do calçado. Para o gráfico PDP do tamanho do calçado, consideramos que a altura não está relacionada com o número. Assim, para o tamanho \\(42\\), por exemplo, calculamos a média da distribuição marginal da altura que pode incluir uma pessoa abaixo de \\(1,50 m\\), o que não é realista para um tamanho \\(42\\) de sapato. Na vida real, é improvável que uma pessoa com altura abaixo de \\(1,50 m\\) use esse tamanho de sapato, ou seja, criamos pontos de dados em área da distribuição das variáveis para os quais é muito baixa a probabilidade real de acontecer. Uma solução para esse problema seria considerar a distribuição condicional da variável, que é o método abordado pelo gráfico de efeito local acumulado, ou gráficos ALE (Apley and Zhu 2016) que tratamos também nesse trabalho. 4.2 Esperança Condicional Individual (Individual Conditional Expectation) - ICE Um gráfico equivalente ao PDP, mas com uma visão individual para cada observação da base de dados é o ICE, apresentado em (Goldstein et al. 2015). Enquanto o PDP tem uma curva média para representar o efeito de uma ou duas variáveis, no ICE temos uma curva para cada indivíduo. O cálculo do ICE é realizado mantendo os valores das demais variáveis e modificando apenas o da variável escolhida para obter as novas predições através do modelo já ajustado. É interessante utilizar o ICE em análise conjunta com o PDP, pois o PDP mostra como é a relação média entre uma variável e a predição, mas como já discutimos, ele funciona bem se as interações são fracas entre as variáveis para as quais o PDP é calculado e as demais variáveis. Entretanto, se houver o cenário de fortes interações, ele omite o relacionamento heterogêneo com diferentes grupos de observação que pode ser observado no ICE. Definindo matematicamente as curvas ICE, seria que para cada elemento \\(i\\) em \\(\\{(x_S^{(i)},x_C^{(i)}\\}_{i=1}^N\\) uma curva \\(\\hat{f}_S^{(i)}\\) é calculada em \\(x_S^{(i)}\\), enquanto \\(x_C^{(i)}\\) é mantido fixo. Vemos também em (Goldstein et al. 2015) uma recomendação de utilizar um agrupamento das curvas ICE, ou seja, ao invés de considerar uma curva para cada indivíduo consideramos \\(k\\) curvas dos indivíduos similarmente agrupados. Essa clusterização está disponível no pacote ICEbox, implementado pelos autores do artigo. No presente trabalho, seguimos essa recomendação e utilizamos a visualização considerando clusters na análise dos gráficos ICE. 4.2.1 Vantagens e Desvantagens Uma vantagem desse método é o quanto ele é de fácil compreensão, sendo que cada linha representa diretamente uma observação da base de dados. O método trás também uma visão sobre o conjunto das observações, podendo agregar informações de subgrupos mais similares, por exemplo. Dependendo do tamanho do conjunto de dados e de sua distribuição, e por trazer uma visualização do comportamento individual, os gráficos ICE podem ficar com muita informação no gráfico, sendo pouco informativos nesse cenário. Por esse mesmo motivo, analisar mais que uma variável com relação ao predito não é recomendado. 4.3 Efeitos Acumulados Locais (Accumulated Local Effects) - ALE Em (Molnar 2019) temos uma comparação muito interessante e didática das técnicas PDP e ALE. No PDP, há o cenário de mostrar o que, em média, é predito pelo modelo quando cada observação tem um valor \\(v\\) de determinada variável, independentemente se aquele valor \\(v\\) faz sentido no contexto das demais varáveis ou não. No entanto, com o ALE, o cenário é analisar como o valor predito pelo modelo muda considerando uma ``pequena janela’’ da variável em torno de um valor \\(v\\), fixando as demais variáveis. Se no PDP calculamos a média das previsões condicionais para cada valor da variável explicativa em análise, no cálculo do ALE, consideramos a distribuição marginal em cada valor do intervalo que ela varia. Em outras palavras, isso significa que temos que definir uma vizinhança. Por exemplo, para o cálculo do efeito de um sapato tamanho \\(40\\) no valor predito das vendas, podemos calcular a média das previsões de todos os sapatos vendidos entre \\(38\\) e \\(42\\). Na Figura é ilustrado exatamente o cenário em que temos duas variáveis correlacionadas. Nesse caso, \\(x_1\\) é correlacionada com \\(x_2\\). Primeiro, a variável \\(x_1\\) é separada em intervalos representados pelas linhas verticais, \\(([z_{0,1},z_{1,1}],[z_{1,1},z_{2,1}])\\), por exemplo. Para os pontos em um determinado intervalo, é calculada a diferença na predição substituindo tais pontos originais pelos extremos do intervalo em que eles estão. Essas diferenças são posteriormente acumuladas e centralizadas. Em outras palavras, os gráficos de ALE calculam a média das alterações nas previsões e as acumulam nesse intervalo definido, que é menor que o intervalo de possíveis valores que a variável assume (Apley and Zhu 2016). Retomando os subconjuntos de variáveis \\(x_S\\) e \\(x_C\\), tal que \\(x_S\\) é um subconjunto das variáveis explicativas de \\(X\\) que utilizaremos no gráfico, e \\(x_C\\) o subconjunto complementar das variáveis utilizadas no ajuste do modelo, temos: \\[ \\hat{f}_{x_S,ALE} (x_S) = \\int_{z_{0,1}}^{x_S} E_{x_C|x_S}[\\hat{f}^S(X)|x_S = z_S] \\partial_{z_S} - constant \\\\ = \\int_{z_{0,1}}^{x_S} \\int_{x_C} \\hat{f}^S(z_S,x_C)P(x_C|z_S) \\delta_{z_S}\\delta_{z_S} - constant \\] Com a integral sobre \\(z\\), acumulamos os gradientes locais ao longo do intervalo do subconjunto \\(x_S\\), o que nos trás o efeito das alterações na predição. Outro ponto é que ao subtrairmos uma constante dos resultados, centralizamos o gráfico ALE, assim o efeito médio sobre os dados é zero. Uma das vantagens desse método é a facilidade de entendimento. Visualmente, ele representa o efeito relativo na predição quando alteramos uma variável, condicionado em um determinado valor. Outra vantagem é que os gráficos ALE não são enviesados, pois funcionam quando variáveis são correlacionadas. Entretanto, dependendo do número de intervalos, os gráficos ALE podem ficar um pouco instáveis. Um número alto de intervalos tendem a ter muitos pequenos altos e baixos. Reduzir o número de intervalos seria uma alternativa, pois torna as estimativas mais estáveis. Entretanto, ele pode suavizar a complexidade do modelo de predição. Temos então um entre um número alto ou baixo de intervalos e não existe uma solução perfeita para definir o número de intervalos . Outro ponto interessante é que, se compararmos ao PDP, temos as curvas ICE que o complementa e ajuda a entender a variação das predições na individualidade do conjunto de dados, observando o efeito diferente para subconjuntos dos dados. Nos gráficos ALE, só se pode verificar por intervalo se o efeito é diferente, e não é possível analisar individualmente com um gráfico auxiliar o mesmo efeito. References "],["interplocal.html", "Capítulo 5 Interpretabilidade Local", " Capítulo 5 Interpretabilidade Local Técnicas de interpretabilidade local facilitam o entendimento de uma perspectiva micro do aprendizado de máquina entre o predito e as variáveis explicativas, como a relação entre a predição de uma observação e suas características correspondentes. Além de trabalharmos com perturbações nas variáveis explicativas, como nos métodos anteriores, uma abordagem interessante é a estimação de modelos mais simples, a partir das variáveis utilizadas no modelo e de sua predição, para explicar algumas predições específicas, que é o caso do LIME . O nome do método pode ser entendido por partes: Basicamente, ao querer entender porque o modelo Caixa Preta'' fez uma certa predição, o LIME testa o que acontece com as predições quando você fornece variações de seus dados no modelo. Com essas amostras de valores perturbados e suas predições é gerado um novo conjunto de dados no qual é treinado um novo modelo, mas dessa vez um modelo interpretável, que será ponderado pela proximidade das unidades amostradas com a predição de interesse estabelecida desde o começo. É importante ressaltar que esse novo modelo ajustado deve ser uma boa aproximação local das predições do modeloCaixa Preta’’, mas não precisa ser uma boa aproximação global e se ajustar a todo o conjunto de dados. Podemos definir a explicação local para uma predição \\(x\\), com o modelo \\(g\\) que minimiza a perda \\(L\\) medindo o quão perto ele está do modelo original \\(f\\) e mantendo o nível de complexidade \\(\\Omega(g)\\) baixo. \\[\\begin{equation} \\text{explicação}(x) = \\arg\\min_{g \\in G} L(f,g,\\pi_x) + \\Omega(g). \\end{equation}\\] O exemplo sobre o classificador de e Lobos citado em ilustra bem o resultado do LIME usando imagens. Na Figura temos um exemplo de um cachorro que foi classificado pelo algoritmo como Lobo. Entretanto, quando utilizado o LIME para entender o porquê da predição é separado como importantes para aquela predição as áreas da imagem que contém neve, conforme destacado. Ou seja, o algoritmo não focava em alguma característica dos animais para diferenciá-los, mas sim no plano de fundo das imagens. As vantagens do algoritmo é que ele é fácil interpretar seus resultados, e pode ser aplicado em dados tabulados, imagens ou textos. A medida de fidelidade resultante do LIME nos permite mensurar de fato quão próximo do modelo original está o modelo ajustado localmente. Por outro lado, a definição de vizinhança é uma desvantagem desse método, uma vez que esse é um conceito abstrato e difícil de ser definido. Como as explicações dependem desse conceito, uma pequena variação nele pode resultar em conclusões diferentes. O algoritmo baseado nos valores de é um método da Teoria de Jogos, apresentado em , que propõe uma solução de distribuição justa de ganhos e custos para vários jogadores que trabalham cooperativamente. Ela se aplica, principalmente, em situações em que as contribuições de cada jogador são desiguais, mas trabalham com o mesmo objetivo. A conexão entre essa solução de Teoria de Jogos e a interpretabilidade de modelos de aprendizado de máquina acontece considerando o jogo como uma tarefa de previsão para uma única observação do conjunto de dados. O ganho é a diferença entre a predição real para a observação e a previsão média para todas as demais observações. Além disso, os jogadores são os valores das variáveis para a observação selecionada, que colaboram para receber o ganho, em outras palavras, para prever um determinado valor. Definiremos como é feita a estimativa aproximada de para o valor de uma única característica, ou seja, primeiro selecionamos uma unidade observacional \\(x\\) do que temos interesse em interpretar a predição obtida, depois para cada variável \\(j\\) do modelo calculamos seu valor Shapley, definindo um número de iterações \\(M\\). Para realizar o cálculo, precisamos definir então: Definidos esses valores, conforme apresentado em , os passos para o cálculo do valor Shapley da \\(j\\)-ésima variável são: Em cada iteração, uma observação aleatória z é selecionada a partir dos dados e uma ordem aleatória das variáveis é gerada, após isso duas novas instâncias são criadas combinando valores da observação \\(x\\) e da observação aleatória \\(z\\). No passo 5 calculamos a contribuição marginal da \\(j\\)-ésima variável pela diferença de \\(f(x_{+j})\\) por \\(f(x_{-j})\\), e após repetir esse cálculo para cada iteração computamos o valor médio da contribuição marginal. Como citado anteriormente, essa sequência de passos deve ser repetida para uma das variáveis, a fim de calcularmos o valor Shapley de cada uma delas. Considere um problema relacionado ao empréstimo de dinheiro. Suponha que dois clientes buscaram um empréstimo e temos as seguintes informações sobre eles: Com as informações da Figura temos que o Cliente 1 possui cartão de crédito, imóvel próprio e não teve dívida recente e o Cliente 2 possui cartão de crédito, imóvel próprio, mas teve uma dívida recente. Como resultado do modelo utilizado para atribuição de um limite de empréstimo o Cliente 1 recebeu R$ 1.500,00 e o Cliente 2 recebeu R$ 800,00. Nesse exemplo, o Cliente 2 teve as três variáveis consideradas conjuntamente para a estimativa do valor de empréstimo de R$ 800,00 e o interessante é entender o que contribuiu para que esse valor ficasse R$ 200,00 abaixo da média (considerando a média de empréstimo igual a R$ 1.000,00). Com as estimativas dos valores de poderíamos dizer, por exemplo, que possuir cartão de crédito contribuiu positivamente R$ 50,00, imóvel próprio contribuiu com R$ 100,00 e histórico de dívida recente contribuiu negativamente -R$ 350,00. Sumarizando as contribuições, temos um saldo de -R$ 200,00. Como vantagens desse método podemos citar que a predição média é razoavelmente distribuída entre os valores das variáveis de uma observação, o que fornece uma explicação completa. Em o autor cita que essa medida é utilizada quando uma interpretação é exigida legalmente na União Europeia, por exemplo, devido a sua robustez. A complexidade computacional do cálculo é uma das desvantagens, sendo viável na maioria dos problemas apenas uma solução aproximada, na nossa aplicação por exemplo, para calcular os valores Shapley (utilizando um computador de 8GB de memória RAM) para uma observação, demorou aproximadamente 150 minutos. Outro ponto que pode pesar de forma negativa nessa técnica é a compreensão do seu resultado, que muitas vezes é mal interpretado como se o valor da variável fosse a predição depois de remover a variável do modelo e retreina-lo. "],["conclusao.html", "Capítulo 6 Conclusões e Discussões", " Capítulo 6 Conclusões e Discussões Entendemos que os métodos de interpretabilidade são muito úteis quando utilizamos técnicas de modelagem não interpretáveis, tanto os métodos mais visuais quanto os que estimam outro modelo, por exemplo, nos ajudam a entender o porquê da predição e desmistificar qual o impacto das variáveis explicativas na predição final. Com os métodos visuais PDP, ICE e ALE temos a facilidade no entendimento de como as variáveis impactam na predição. O LIME e os valores nos permitem analisar mais minuciosamente o porque dos valores preditos. A combinação dessas técnicas é muito informativa, pois avaliamos a questão de interpretabilidade de uma maneira mais completa e de diferentes pontos da explicação. É importante ressaltar que mesmo com o auxílio de ferramentas de interpretabilidade, o uso de modelos opacos precisa ser avaliado com cautela, pois como citado em , algumas vezes tem-se a crença que esses modelos têm a capacidade de descobrir padrões ocultos nos dados, mas em muitos casos um modelo transparente pode ser capaz de descobrir esses mesmos padrões, sendo importante considerar o processo geral do desenvolvimento de um modelo, desde a seleção dos dados e os tratamentos das variáveis, e não apenas qual a técnica de modelagem será utilizada. No próximo capítulo traremos uma aplicação em dados de um problema , considerando um modelo interpretável e um modelo opaco, a fim de observar os resultados das duas diferentes abordagens. "],["references.html", "References", " References "]]
